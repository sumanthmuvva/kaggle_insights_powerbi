{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7599329,"sourceType":"datasetVersion","datasetId":1810455},{"sourceId":8199263,"sourceType":"datasetVersion","datasetId":9},{"sourceId":8217776,"sourceType":"datasetVersion","datasetId":4871088}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cleaning UserAchievements.csv file\nimport pandas as pd\nimport os\nimport numpy as np\n\n# Load the UserAchievements.csv file\ndf = pd.read_csv('/kaggle/input/meta-kaggle/UserAchievements.csv')\nprint(\"Reading completed..\\n\")\n\n# Filter out rows where Points, TotalGold, and TotalSilver are all zero\ndf_filtered = df[~((df['Points'] == 0) & (df['TotalGold'] == 0) & (df['TotalSilver'] == 0))]\nprint(\"Filtering completed..\\n\")\n\n# Save the filtered DataFrame to a new CSV file in the Kaggle working directory\ndf_filtered.to_csv('/kaggle/working/UserAchievements_Cleaned.csv', index=False)\nprint(\"Saving to --> /kaggle/working/UserAchievements_Cleaned.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:04:15.644548Z","iopub.execute_input":"2024-03-28T01:04:15.645234Z","iopub.status.idle":"2024-03-28T01:07:11.650276Z","shell.execute_reply.started":"2024-03-28T01:04:15.645180Z","shell.execute_reply":"2024-03-28T01:07:11.648577Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Reading completed..\n\nFiltering completed..\n\nSaving to --> /kaggle/working/UserAchievements_Cleaned.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Getting Metadata of the files\n\nimport pandas as pd\nimport os\nimport numpy as np\n\n# Define the directory where your dataset is located\ndataset_directory = '/kaggle/input/meta-kaggle/'  # Update this path to your specific dataset directory\n\n# Output directory for Excel files\noutput_directory = '/kaggle/working/'  # This is a common output directory in Kaggle kernels\n\n# List of specific filenames you want to process\nfilenames_to_process = [\n    \"UserAchievements_Cleaned.csv\",\n    \"Submissions.csv\",\n    \"Users.csv\",\n    \"ForumMessages.csv\",\n    \"Teams.csv\",\n    \"UserFollowers.csv\",\n    \"ForumMessageVotes.csv\",\n    \"ForumTopics.csv\",\n    \"KernelTags.csv\",\n    \"Datasets.csv\",\n    \"DatasetVersions.csv\",\n    \"DatasetTags.csv\",\n    \"Forums.csv\",\n    \"Competitions.csv\",\n    \"DatasetTaskSubmissions.csv\",\n    \"DatasetTasks.csv\",\n    \"UserOrganizations.csv\",\n    \"Tags.csv\",\n    \"Organizations.csv\",\n    \"CompetitionTags.csv\",\n    \"KernelLanguages.csv\"\n]\n#tables = []\nmetadata = []\n\nfor filename in filenames_to_process:\n    print(f\"Processing {filename}...\")\n    # Special case for the cleaned UserAchievements.csv since it's in a different directory.\n    if filename == \"UserAchievements_Cleaned.csv\":\n        df = pd.read_csv(os.path.join(output_directory, filename))\n    else:\n        df = pd.read_csv(os.path.join(dataset_directory, filename))\n    table_name = filename.split('.')[0]\n\n    # Iterate through each column to gather metadata\n    for column in df.columns:\n        col_data = df[column]\n        meta = {\n            'table_name': table_name,\n            'column_name': column,\n            'data_type': col_data.dtype,\n            'non_null_count': col_data.notnull().sum(),\n            'unique_count': col_data.nunique()\n        }\n\n        # Additional statistics for numerical columns\n        if pd.api.types.is_numeric_dtype(col_data):\n            meta['min'] = col_data.min()\n            meta['max'] = col_data.max()\n            meta['mean'] = col_data.mean()\n            meta['median'] = col_data.median()\n            meta['std_dev'] = col_data.std()\n\n        # Additional information for categorical columns\n        if pd.api.types.is_categorical_dtype(col_data) or pd.api.types.is_object_dtype(col_data):\n            top_categories = col_data.value_counts().nlargest(5).index.tolist()\n            meta['top_categories'] = top_categories\n\n        metadata.append(meta)\n\n    #tables.append(df)\n    print(f\"Completed processing {filename}.\\n\")\n\n# Concatenate all tables horizontally (you can change axis to 0 for vertical concatenation)\n#result = pd.concat(tables, axis=1)\n\n# Convert metadata list to a DataFrame for easier viewing and analysis\nmetadata_df = pd.DataFrame(metadata)\n\n# Save the concatenated data and metadata to Excel files\n#result_file_path = os.path.join(output_directory, 'concatenated_data.xlsx')\nmetadata_file_path = os.path.join(output_directory, 'metadata.xlsx')\n\n#print(\"Saving concatenated data to Excel...\")\n#result.to_excel(result_file_path, index=False)\n\nprint(\"Saving metadata to Excel...\")\nmetadata_df.to_excel(metadata_file_path, index=False)\n\nprint(f\"Metadata file created successfully at : {metadata_file_path}\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning Submissions.csv file\nimport pandas as pd\n\nfile_path = '/kaggle/input/meta-kaggle/Submissions.csv'\noutput_path = '/kaggle/working/Submissions_Cleaned.csv'\n\n# Load the CSV file with only the specified columns\ncolumns = ['Id', 'SubmittedUserId', 'TeamId', 'PublicScoreLeaderboardDisplay', 'SubmissionDate', 'IsAfterDeadline']\ndf = pd.read_csv(file_path, usecols=columns)\n\n# Convert 'SubmissionDate' to datetime format to ensure correct sorting\ndf['SubmissionDate'] = pd.to_datetime(df['SubmissionDate'])\n\n# Sort by 'SubmittedUserId' and 'SubmissionDate' to get the latest submission by each user\ndf_sorted = df.sort_values(by=['SubmittedUserId', 'SubmissionDate'], ascending=[True, False])\n\n# Drop duplicate 'SubmittedUserId', keeping the first (latest submission based on 'SubmissionDate')\ndf_cleaned = df_sorted.drop_duplicates(subset='SubmittedUserId', keep='first')\n\n# Save the cleaned DataFrame to a new CSV file\ndf_cleaned.to_csv(output_path, index=False)\n\nprint(f'Cleaned file saved to: {output_path}')","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:05:31.994286Z","iopub.execute_input":"2024-03-28T14:05:31.994784Z","iopub.status.idle":"2024-03-28T14:06:22.823197Z","shell.execute_reply.started":"2024-03-28T14:05:31.994751Z","shell.execute_reply":"2024-03-28T14:06:22.821847Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cleaned file saved to: /kaggle/working/Submissions_Cleaned.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cleaning UserFollowers.csv file\nimport pandas as pd\n\n# Load the UserFollowers.csv file\nfile_path = '/kaggle/input/meta-kaggle/UserFollowers.csv'\noutput_path = '/kaggle/working/UserFollowers_Cleaned.csv'\n\n# Assuming the fields 'UserId', 'FollowingUserId', and 'CreationDate' exist in your dataset\ndf = pd.read_csv(file_path)\n\n# Convert 'CreationDate' to datetime to ensure correct sorting\ndf['CreationDate'] = pd.to_datetime(df['CreationDate'])\n\n# First, sort by 'UserId' and 'CreationDate' to prepare for dropping duplicates\ndf_sorted = df.sort_values(by=['UserId', 'CreationDate'], ascending=[True, False])\n\n# Drop duplicates based on 'UserId' and 'FollowingUserId', keeping the latest entry\ndf_deduplicated = df_sorted.drop_duplicates(subset=['UserId', 'FollowingUserId'], keep='first')\n\n# Now, create a new DataFrame to count followers per 'UserId'\ndf_followers_count = df_deduplicated.groupby('UserId').size().reset_index(name='Followers_Count')\n\n# Save the cleaned and aggregated DataFrame to a new CSV file\ndf_followers_count.to_csv(output_path, index=False)\n\nprint(f'Cleaned file with followers count saved to: {output_path}')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:28:30.703463Z","iopub.execute_input":"2024-03-28T03:28:30.705798Z","iopub.status.idle":"2024-03-28T03:28:33.627647Z","shell.execute_reply.started":"2024-03-28T03:28:30.705716Z","shell.execute_reply":"2024-03-28T03:28:33.626359Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Cleaned file with followers count saved to: /kaggle/working/UserFollowers_Cleaned.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cleaning Teams.csv file\nimport pandas as pd\n\n# Load Teams.csv\nteams_file_path = '/kaggle/input/meta-kaggle/Teams.csv'\nteams = pd.read_csv(teams_file_path, usecols=[\n    'Id', 'CompetitionId', 'TeamLeaderId', 'TeamName', 'LastSubmissionDate', \n    'PublicLeaderboardSubmissionId', 'Medal', 'PublicLeaderboardRank'\n])\n\n# Load Submissions_Cleaned.csv\nsubmissions_cleaned_file_path = '/kaggle/working/Submissions_Cleaned.csv'\nsubmissions_cleaned = pd.read_csv(submissions_cleaned_file_path)\n\n# Filter out rows from Teams where the specified columns are all null\nteams_cleaned = teams.dropna(\n    subset=['LastSubmissionDate', 'PublicLeaderboardSubmissionId', 'Medal', 'PublicLeaderboardRank'],\n    how='all'\n)\n\n# Ensure that Id from Teams is not present as TeamId in Submissions_Cleaned\nteams_cleaned = teams_cleaned[~teams_cleaned['Id'].isin(submissions_cleaned['TeamId'])]\n\n# Save the cleaned Teams data\noutput_path = '/kaggle/working/Teams_Cleaned.csv'\nteams_cleaned.to_csv(output_path, index=False)\n\nprint(f\"Cleaned Teams data saved to {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:06:38.115563Z","iopub.execute_input":"2024-03-28T14:06:38.116027Z","iopub.status.idle":"2024-03-28T14:07:02.166291Z","shell.execute_reply.started":"2024-03-28T14:06:38.115994Z","shell.execute_reply":"2024-03-28T14:07:02.164905Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cleaned Teams data saved to /kaggle/working/Teams_Cleaned.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cleaning Teams.csv file\nimport pandas as pd\nimport re\n\n# Paths for the files\nteams_file_path = '/kaggle/input/meta-kaggle/Teams.csv'\nsubmissions_cleaned_file_path = '/kaggle/working/Submissions_Cleaned.csv'\noutput_path = '/kaggle/working/Teams_Cleaned1.csv'\n\n# Load the Teams.csv file\nteams = pd.read_csv(teams_file_path, usecols=[\n    'Id', 'CompetitionId', 'TeamLeaderId', 'TeamName', 'LastSubmissionDate', \n    'PublicLeaderboardSubmissionId', 'Medal', 'PublicLeaderboardRank'\n])\n\n# Load the Submissions_Cleaned.csv file\nsubmissions_cleaned = pd.read_csv(submissions_cleaned_file_path)\n\n# Define the regex pattern for allowed characters in TeamName\n# Add or remove characters from the pattern as needed\nallowed_chars_pattern = re.compile(r'^[a-zA-Z0-9\\s\\-#@$_,\\']+$')\n\n# Apply filters to the teams DataFrame\nteams_cleaned = teams[\n    # Filter out '[Deleted]' in TeamName\n    ~teams['TeamName'].str.contains('\\[Deleted\\]', na=False) &\n    # Keep only rows with allowed characters in TeamName\n    teams['TeamName'].apply(lambda x: bool(allowed_chars_pattern.match(x)) if pd.notna(x) else True) &\n    # Filter out null columns\n    teams[['LastSubmissionDate', 'PublicLeaderboardSubmissionId', 'Medal', 'PublicLeaderboardRank']].notnull().any(axis=1)\n]\n\n# Ensure that Id from Teams is not present as TeamId in Submissions_Cleaned\nteams_cleaned = teams_cleaned[~teams_cleaned['Id'].isin(submissions_cleaned['TeamId'])]\n\n# Save the cleaned Teams data to a new CSV file\nteams_cleaned.to_csv(output_path, index=False)\n\nprint(f\"Cleaned Teams data saved to {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T14:15:58.731027Z","iopub.execute_input":"2024-03-28T14:15:58.734483Z","iopub.status.idle":"2024-03-28T14:16:49.856102Z","shell.execute_reply.started":"2024-03-28T14:15:58.734388Z","shell.execute_reply":"2024-03-28T14:16:49.854794Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cleaned Teams data saved to /kaggle/working/Teams_Cleaned1.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculating sentiment polarity for messages \nimport pandas as pd\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/input/meta-kaggle/ForumMessages.csv')\noutput_path = '/kaggle/working/ForumMessages_Cleaned.csv'\n\n# Drop duplicates across the entire dataframe\ndf = df.drop_duplicates()\n\n# Convert 'MessageDate' to date format and remove the time part\ndf['MessageDate'] = pd.to_datetime(df['MessageDate']).dt.date\n\n# Initialize the VADER sentiment intensity analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Define a function to get the compound polarity score\ndef get_compound_polarity(text):\n    try:\n        scores = sid.polarity_scores(str(text))  # Ensure text is treated as a string\n        return scores['compound']\n    except Exception as e:\n        print(f\"Error processing text: {e}\")\n        return None\n\n# Apply the function to your message column to create a new 'polarity_score' column\ndf['polarity_score'] = df['Message'].apply(get_compound_polarity)\n\n# Drop the 'Message' column as it's no longer needed after computing the sentiment scores\ndf = df.drop(columns=['Message'])\n\n# Also drop other specified columns\ncolumns_to_drop = ['PostUserId', 'ReplyToForumMessageId', 'Medal', 'MedalAwardDate']\ndf = df.drop(columns=columns_to_drop)\n\n# Save the modified dataframe back to a CSV file\ndf.to_csv(output_path, index=False)\n\nprint(\"Cleaning and saving process completed.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning ForumMessages.csv file...\nimport pandas as pd\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/working/ForumMessages_Cleaned.csv')\noutput_path = '/kaggle/working/ForumMessages.csv'\n\n# Drop the 'Message' column as it's no longer needed after computing the sentiment scores\ndf = df.drop(columns=['Message'])\n\n\n\n# Convert 'MessageDate' to date format and remove the time part\ndf['PostDate'] = pd.to_datetime(df['PostDate']).dt.date\n\n# Drop duplicates across the entire dataframe\ndf = df.drop_duplicates()\n\n# Define a function to categorize sentiment based on 'polarity_score'\ndef categorize_sentiment(score):\n    if score < 0:\n        return 'Negative'\n    elif score > 0:\n        return 'Positive'\n    else:\n        return 'Neutral'\n\n# Apply the function to create a new 'sentiment' column\ndf['sentiment'] = df['polarity_score'].apply(categorize_sentiment)\n\n# Save the modified dataframe back to a CSV file\ndf.to_csv(output_path, index=False)\n\nprint(\"Cleaning and saving process completed.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T20:55:43.475805Z","iopub.execute_input":"2024-03-28T20:55:43.476350Z","iopub.status.idle":"2024-03-28T20:56:32.026909Z","shell.execute_reply.started":"2024-03-28T20:55:43.476312Z","shell.execute_reply":"2024-03-28T20:56:32.024829Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Cleaning and saving process completed.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Merging Users with MasterProfiles, Master Acheivements data to get locations and Id into one file.\nimport pandas as pd\n\n# Load the first Excel file\nfile_path1 = '/kaggle/input/meta-kagglemaster-achievements-snapshot/MasterAchievements.csv'\ndf1 = pd.read_csv(file_path1)\n\n# Load the second Excel file\nfile_path2 = '/kaggle/input/meta-kagglemaster-achievements-snapshot/MasterProfiles.csv'\ndf2 = pd.read_csv(file_path2)\n\n# Load the Users.csv file\nusers_file_path = '/kaggle/input/meta-kaggle/Users.csv'\nusers_df = pd.read_csv(users_file_path)\n\n# Merge the two DataFrames on 'UserName'\n# This performs a left join by default, getting all records from df1\n# and the matching records from df2, based on 'UserName'\n# Change 'how' argument to 'inner', 'right', or 'outer' as per your requirement\nmaster_profiles_df = pd.merge(df1, df2, on='UserName', how='left')\n\n# Merge the two DataFrames on 'UserName' to get the 'Id' column from the Users table\n# The default merge is an inner join, which will only keep records that have matching 'UserName' in both tables\n# If you want to keep all records from MasterProfiles and just add matching Ids from Users, you should use how='left'\nmerged_df = pd.merge(master_profiles_df, users_df[['Id', 'UserName']], on='UserName', how='left')\n\n# Save the merged DataFrame to a new CSV file\noutput_path = '/kaggle/working/MasterProfiles.csv'\nmerged_df.to_csv(output_path, index=False)\n\nprint(f\"Merged data saved to {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T15:47:38.464649Z","iopub.execute_input":"2024-04-24T15:47:38.465033Z","iopub.status.idle":"2024-04-24T15:48:35.898596Z","shell.execute_reply.started":"2024-04-24T15:47:38.465002Z","shell.execute_reply":"2024-04-24T15:48:35.897291Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Merged data saved to /kaggle/working/MasterProfiles.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Map performance tier to GM, Master, Expert, Contributor, Novice, Staff\nimport pandas as pd\n\n# Assuming 'Users.csv' is in the Kaggle input directory. Adjust the path if it's different.\nusers_file_path = '/kaggle/input/users-csv/Users.csv'\noutput_path = '/kaggle/working/Users.csv'\n\n# Load the Users.csv file\nusers_df = pd.read_csv(users_file_path)\n\n# Create a mapping of performance tier numbers to names\nperformance_tier_mapping = {\n    0: 'novice',\n    1: 'contributor',\n    2: 'expert',\n    3: 'master',\n    4: 'grandmaster',\n    5: 'staff'\n}\n\n# Map the 'PerformanceTier' to 'TierName'\nusers_df['TierName'] = users_df['PerformanceTier'].map(performance_tier_mapping)\n\n# Save the updated DataFrame to a new CSV file\nusers_df.to_csv(output_path, index=False)\n\nprint(f\"Updated Users data with TierName saved to {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T16:00:35.972292Z","iopub.execute_input":"2024-04-24T16:00:35.972908Z","iopub.status.idle":"2024-04-24T16:00:39.981463Z","shell.execute_reply.started":"2024-04-24T16:00:35.972878Z","shell.execute_reply":"2024-04-24T16:00:39.980450Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Updated Users data with TierName saved to /kaggle/working/Users.csv\n","output_type":"stream"}]}]}